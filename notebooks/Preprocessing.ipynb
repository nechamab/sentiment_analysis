{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:09:51.256499Z",
     "start_time": "2024-02-11T17:09:50.959257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nechamaborisute/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nechamaborisute/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:11:05.245984Z",
     "start_time": "2024-02-11T17:11:00.659845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048572 entries, 0 to 1048571\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   sentiment  1048572 non-null  int64 \n",
      " 1   tweet_id   1048572 non-null  int64 \n",
      " 2   date       1048572 non-null  object\n",
      " 3   query      1048572 non-null  object\n",
      " 4   user       1048572 non-null  object\n",
      " 5   text       1048572 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 48.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load in data \n",
    "data = pd.read_csv('../data/data.csv.zip', encoding='latin-1')\n",
    "\n",
    "# rename columns\n",
    "data.columns = ['sentiment', 'tweet_id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Look at data information, dtypes, nulls\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:11:28.410288Z",
     "start_time": "2024-02-11T17:11:28.390172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment    tweet_id                          date     query  \\\n",
       "0          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user                                               text  \n",
       "0  scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2        ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3         Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4       joy_wolf                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data file format has 6 fields:\n",
    "* 0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "* 1 - the id of the tweet (2087)\n",
    "* 2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "* 3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "* 4 - the user that tweeted (robotickilldozr)\n",
    "* 5 - the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:11:39.640709Z",
     "start_time": "2024-02-11T17:11:39.632143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    is upset that he can't update his Facebook by ...\n",
       "1    @Kenichan I dived many times for the ball. Man...\n",
       "2      my whole body feels itchy and like its on fire \n",
       "3    @nationwideclass no, it's not behaving at all....\n",
       "4                        @Kwesidei not the whole crew \n",
       "5                                          Need a hug \n",
       "6    @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "7                           I just re-pierced my ears \n",
       "8    @caregiving I couldn't bear to watch it.  And ...\n",
       "9    @octolinz16 It it counts, idk why I did either...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data is messy and has a lot of characters I'd like to remove before analysis, so I'm going to write a function to go over and clean all the tweets.\n",
    "\n",
    "* stop words\n",
    "* punctuation\n",
    "* @, url, user\n",
    "* lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:24:48.233692Z",
     "start_time": "2024-02-11T17:24:48.221628Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet_text, min_length):\n",
    "    \n",
    "    # get common stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # instantiate lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "        \n",
    "    # lower case everything\n",
    "    tweet_lower = tweet_text.lower()\n",
    "    \n",
    "     #remove mentions, hashtags, and urls, strip whitspace and breaks\n",
    "    tweet_lower = re.sub(r\"@[a-z0-9_]+|#[a-z0-9_]+|http\\S+\", \"\", tweet_lower).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    \n",
    "    # remove stop words and punctuations \n",
    "    tweet_norm = [x for x in word_tokenize(tweet_lower) if ((x.isalpha()) & (x not in stop_words)) ]\n",
    "\n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(tweet_norm))) \n",
    "\n",
    "    # now we are going to have a cutoff here. any tokenized cocument with length < min length will be removed from corpus\n",
    "    if len(wordnet_tagged) <= min_length:\n",
    "        return ''\n",
    "    else:\n",
    "         # rejoins lemmatized sentence \n",
    "        tweet_norm = \" \".join([wnl.lemmatize(x[0], x[1]) for x in wordnet_tagged if x[1] is not None])\n",
    "        return tweet_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:48:13.015484Z",
     "start_time": "2024-02-11T17:24:48.704058Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply function to tweets\n",
    "data['text'] = data['text'].apply(process_tweet, args = [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:55:03.366221Z",
     "start_time": "2024-02-11T17:55:02.620121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          upset update facebook texting cry result schoo...\n",
       "6          hey long time see yes rain bite bit lol fine t...\n",
       "27         want go promote gear groove unfornately ride b...\n",
       "31         ok sick spend hour sit shower cause sick stand...\n",
       "32                 ill tell ya story later good day ill hour\n",
       "                                 ...                        \n",
       "1048556    watchin espn first take favorite mornin show l...\n",
       "1048558    muhaha thankgoodness miss last date rmcaat jun...\n",
       "1048559    good morning people twitter tgifriday thats wa...\n",
       "1048565    today message church service deliver skype fre...\n",
       "1048566    back home thought do week call alter something...\n",
       "Name: text, Length: 207344, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of empty tweets\n",
    "data_cleaned = data[data['text'] != '']\n",
    "data_cleaned['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:58:20.113205Z",
     "start_time": "2024-02-11T17:58:18.723160Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_cleaned.to_csv('../data/data_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
